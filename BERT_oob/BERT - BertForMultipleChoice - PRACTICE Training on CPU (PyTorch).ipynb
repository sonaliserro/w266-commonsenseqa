{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - Out of the Box\n",
    "\n",
    "In this notebook, we will test the performance of an out-of-the-box BERT model on CommonsenseQA. I follow the tutorial here: https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
    "\n",
    "I've implemented the Hugginface transformers library. \n",
    "\n",
    "I referred to the Commonsense QA repo and code to understand how the authors of this work establiahsed their baseline using BERT. This is the link to their code: https://github.com/jonathanherzig/commonsenseqa/blob/master/bert/run_commonsense_qa.py\n",
    "\n",
    "From this repo (README): https://github.com/jonathanherzig/commonsenseqa\n",
    "\n",
    "Their work is far more advanced and complicated than maybe what I want to do at this time. But I refer to their work to understand the set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run type: tiny\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "#  PyTorch reimplementation of Google's TensorFlow repository for the BERT model \n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMultipleChoice\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "runtype=\"tiny\"\n",
    "\n",
    "print(\"Run type:\", runtype)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# print(\"Num CPUs Available: \",\n",
    "#       len(tf.config.experimental.list_physical_devices('CPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "It's in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answerKey</th>\n",
       "      <th>id</th>\n",
       "      <th>question_concept</th>\n",
       "      <th>choices</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
       "      <td>punishing</td>\n",
       "      <td>[{'label': 'A', 'text': 'ignore'}, {'label': '...</td>\n",
       "      <td>The sanctions against the school were a punish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
       "      <td>people</td>\n",
       "      <td>[{'label': 'A', 'text': 'race track'}, {'label...</td>\n",
       "      <td>Sammy wanted to go to where the people were.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
       "      <td>choker</td>\n",
       "      <td>[{'label': 'A', 'text': 'jewelry store'}, {'la...</td>\n",
       "      <td>To locate a choker not located in a jewelry bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
       "      <td>highway</td>\n",
       "      <td>[{'label': 'A', 'text': 'united states'}, {'la...</td>\n",
       "      <td>Google Maps and other highway and street GPS s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
       "      <td>fox</td>\n",
       "      <td>[{'label': 'A', 'text': 'pretty flowers.'}, {'...</td>\n",
       "      <td>The fox walked from the city into the forest, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  answerKey                                id question_concept  \\\n",
       "0         A  075e483d21c29a511267ef62bedc0461        punishing   \n",
       "1         B  61fe6e879ff18686d7552425a36344c8           people   \n",
       "2         A  4c1cb0e95b99f72d55c068ba0255c54d           choker   \n",
       "3         D  02e821a3e53cb320790950aab4489e85          highway   \n",
       "4         C  23505889b94e880c3e89cff4ba119860              fox   \n",
       "\n",
       "                                             choices  \\\n",
       "0  [{'label': 'A', 'text': 'ignore'}, {'label': '...   \n",
       "1  [{'label': 'A', 'text': 'race track'}, {'label...   \n",
       "2  [{'label': 'A', 'text': 'jewelry store'}, {'la...   \n",
       "3  [{'label': 'A', 'text': 'united states'}, {'la...   \n",
       "4  [{'label': 'A', 'text': 'pretty flowers.'}, {'...   \n",
       "\n",
       "                                                stem  \n",
       "0  The sanctions against the school were a punish...  \n",
       "1  Sammy wanted to go to where the people were.  ...  \n",
       "2  To locate a choker not located in a jewelry bo...  \n",
       "3  Google Maps and other highway and street GPS s...  \n",
       "4  The fox walked from the city into the forest, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(file):\n",
    "    lines = []\n",
    "    with open(file, 'rb') as json_file:\n",
    "        for json_line in json_file:\n",
    "            lines.append(json.loads(json_line))\n",
    "        data = json_normalize(lines)\n",
    "        data.columns = data.columns.map(lambda x: x.split(\".\")[-1])\n",
    "    return data\n",
    "# os.chdir('w266-commonsenseqa/BERT_oob)\n",
    "train = load_data('../dataset/train_rand_split.jsonl')\n",
    "dev = load_data('../dataset/dev_rand_split.jsonl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Sammy wanted to go to where the people were.  Where might he go?\n",
      "2 To locate a choker not located in a jewelry box or boutique where would you go?\n",
      "3 Google Maps and other highway and street GPS services have replaced what?\n",
      "4 The fox walked from the city into the forest, what was it looking for?\n",
      "5 What home entertainment equipment requires cable?\n",
      "6 The only baggage the woman checked was a drawstring bag, where was she heading with it?\n",
      "7 The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\n",
      "8 What do people use to absorb extra ink from a fountain pen?\n",
      "9 Where is a business restaurant likely to be located?\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,10):\n",
    "    print(n, train.iloc[n].stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Import training examples\n",
    "2. Process it\n",
    "    - Format input into something BERT can work with, including `[CLS]` and `[SEP]`\n",
    "    - We were thinking which label is correct: \n",
    "    - Tokenize \n",
    "    - Create an output layer using softmax. \n",
    "3. Train it\n",
    "    - Specify how many layers of BERT to fine tune\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert for multiple choice\n",
    "\n",
    "Source: https://github.com/google-research/bert/issues/38\n",
    "\n",
    "> Let's assume your batch size is 8 and your sequence length is 128. Each SWAG example has 4 entries, the correct one and 3 incorrect ones.\n",
    "> \n",
    "> Instead of your input_fn returning an input_ids of size [128], it should return one of size [4, 128]. Same for mask and sequence ids. So for each example, you will generate the sequences predicate ending0, predicate ending1, predicate ending2, predicate ending3. Also return a label scalar which is in an integer in the range [0, 3] to indicate what the gold ending is.\n",
    "> \n",
    "> After batching, your model_fn will get an input of shape [8, 4, 128]. Reshape these to [32, 128] before passing them into BertModel. I.e., BERT will consider all of these independently.\n",
    "> \n",
    "> Compute the logits as in run_classifier.py, but your \"classifier layer\" will just be a vector of size [768] (or whatever your hidden size is).\n",
    "> \n",
    "> Now you have a set of logits of size [32]. Re-shape these back into [8, 4] and then compute tf.nn.log_softmax() over the 4 endings for each example. Now you have log probabilities of shape [8, 4] over the 4 endings and a label tensor of shape [8], so compute the loss exactly as you would for a classification problem.\n",
    "\n",
    "\n",
    "Source: https://github.com/huggingface/transformers/pull/96/files\n",
    "\n",
    "> 6. `BertForMultipleChoice`\n",
    ">\n",
    "> `BertForMultipleChoice` is a fine-tuning model that includes `BertModel` and a linear layer on top of the `BertModel`.\n",
    "> \n",
    "> The linear layer outputs a single value for each choice of a multiple choice problem, then all the outputs corresponding to an instance are passed through a softmax to get the model choice.\n",
    "> \n",
    "> This implementation is largely inspired by the work of OpenAI in [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) and the answer of Jacob Devlin in the following [issue](https://github.com/google-research/bert/issues/38).\n",
    "> \n",
    "> An example on how to use this class is given in the [`run_swag.py`](./examples/run_swag.py) script which can be used to fine-tune a multiple choice classifier using BERT, for example for the Swag task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to be following the code in this repo very closely: https://github.com/rodgzilla/pytorch-pretrained-BERT/tree/dcb50eaa4b80d3ab75d373c36780c80fb47cfd97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question, there are five answer choices. Only one of them is correct.\n",
    "\n",
    "For BERT, the first thought was to have all five answers attached to each question, and the model would choose one of the five responses. This is how it's originally done in the CommonsenseQA paper.\n",
    "\n",
    "```\n",
    "[CLS] Question text here [SEP] Ans choice A [SEP] Ans choice B [SEP] Ans choice C [SEP] Ans choice D [SEP] Ans choice E [SEP]\n",
    "```\n",
    "\n",
    "It seems complicated, however, and requires a significant lift. So for now, let me try creating five question-answer pairs for each question. Like this:\n",
    "\n",
    "```\n",
    "[CLS] Question text here [SEP] Ans choice A [SEP]\n",
    "[CLS] Question text here [SEP] Ans choice B [SEP]\n",
    "[CLS] Question text here [SEP] Ans choice C [SEP]\n",
    "[CLS] Question text here [SEP] Ans choice D [SEP]\n",
    "[CLS] Question text here [SEP] Ans choice E [SEP]\n",
    "```\n",
    "\n",
    "The shape of the input will be 5 rows of tokenized values. The label will have a numerical value to indicate which of the answer choices was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "lab_order = {\"A\": 0, \"B\":1, \"C\":2, \"D\":3, \"E\":4}\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single multiple choice question and its five multiple choice answer candidates\"\"\"\n",
    "    # This class is adapted from https://github.com/jonathanherzig/commonsenseqa/blob/master/bert/run_commonsense_qa.py\n",
    "    # and from https://github.com/rodgzilla/pytorch-pretrained-BERT/blob/dcb50eaa4b80d3ab75d373c36780c80fb47cfd97/examples/run_swag.py\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            qid,\n",
    "            question,\n",
    "            choice_0,\n",
    "            choice_1,\n",
    "            choice_2,\n",
    "            choice_3,\n",
    "            choice_4,\n",
    "            label=None):\n",
    "        \"\"\"Construct an instance.\"\"\"\n",
    "        self.qid = qid\n",
    "        self.question = question  # e.g., 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?'\n",
    "        self.choices = [          # All five anser choices as a list\n",
    "            choice_0,\n",
    "            choice_1,\n",
    "            choice_2,\n",
    "            choice_3,\n",
    "            choice_4\n",
    "        ]\n",
    "        self.label = label        # \n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        l = [\n",
    "            f\"qid: {self.qid}\",\n",
    "            f\"question: {self.question}\",\n",
    "            f\"choice_0: {self.choices[0]}\",\n",
    "            f\"choice_1: {self.choices[1]}\",\n",
    "            f\"choice_2: {self.choices[2]}\",\n",
    "            f\"choice_3: {self.choices[3]}\",\n",
    "            f\"choice_4: {self.choices[4]}\",\n",
    "        ]\n",
    "\n",
    "        if self.label is not None:\n",
    "            l.append(f\"label: {self.label}\")\n",
    "\n",
    "        return \", \".join(l)    \n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"Adapted from: https://github.com/rodgzilla/pytorch-pretrained-BERT/blob/dcb50eaa4b80d3ab75d373c36780c80fb47cfd97/examples/run_swag.py\n",
    "    Stores Bert model inputs (ids, masks) for each example\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 choices_features,\n",
    "                 label\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.choices_features = [\n",
    "            {\n",
    "                'input_ids': input_ids,\n",
    "                'input_mask': input_mask,\n",
    "                'segment_ids': segment_ids\n",
    "            }\n",
    "            for _, input_ids, input_mask, segment_ids in choices_features\n",
    "        ]\n",
    "        self.label = label\n",
    "\n",
    "    \n",
    "def process_examples(data):\n",
    "    \"\"\"Given the examples in a pandas df format, process examples into example class\"\"\"\n",
    "    examples = []\n",
    "    labels = []\n",
    "    questions = []\n",
    "    anscands = []\n",
    "    \n",
    "    \n",
    "    for index, row in data.iterrows(): \n",
    "        example = InputExample(\n",
    "                    qid=row.id,\n",
    "                    question=row.stem,\n",
    "                    choice_0=str(row.choices[0]).replace(\"'\",\"\"),\n",
    "                    choice_1=str(row.choices[1]).replace(\"'\",\"\"),\n",
    "                    choice_2=str(row.choices[2]).replace(\"'\",\"\"),\n",
    "                    choice_3=str(row.choices[3]).replace(\"'\",\"\"),\n",
    "                    choice_4=str(row.choices[4]).replace(\"'\",\"\"),\n",
    "                    label=lab_order[row.answerKey]\n",
    "                )\n",
    "        examples.append(example)\n",
    "        \n",
    "    return examples \n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length, is_training):\n",
    "    # For each quesiton, we generate five inputs: one for each answer choice. \n",
    "    \n",
    "    # - [CLS] question [SEP] choice_1 [SEP]\n",
    "    # - [CLS] question [SEP] choice_2 [SEP]\n",
    "    # - [CLS] question [SEP] choice_3 [SEP]\n",
    "    # - [CLS] question [SEP] choice_4 [SEP]\n",
    "    # - [CLS] question [SEP] choice_5 [SEP]\n",
    "    \n",
    "    features = []\n",
    "    # Loop through questions\n",
    "    for example_index, example in enumerate(examples):\n",
    "        question_tokens = tokenizer.tokenize(example.question)\n",
    "\n",
    "        choices_features = []\n",
    "        # For each question, loop through all answer choices \n",
    "        for choice_index, choice in enumerate(example.choices):\n",
    "            # We create a copy of the question tokens in order to be\n",
    "            # able to shrink it according to choice_tokens\n",
    "            question_tokens_choice = question_tokens[:]\n",
    "            choice_tokens = tokenizer.tokenize(choice)\n",
    "            # Modifies `question_tokens_choice` and `choice_tokens` in\n",
    "            # place so that the total length is less than the\n",
    "            # specified length.  Account for [CLS], [SEP], [SEP] with\n",
    "            # \"- 3\"\n",
    "            _truncate_seq_pair(question_tokens_choice, choice_tokens, max_seq_length - 3)\n",
    "\n",
    "            tokens = [\"[CLS]\"] + question_tokens_choice + [\"[SEP]\"] + choice_tokens + [\"[SEP]\"]\n",
    "            segment_ids = [0] * (len(question_tokens_choice) + 2) + [1] * (len(choice_tokens) + 1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding = [0] * (max_seq_length - len(input_ids))\n",
    "            input_ids += padding\n",
    "            input_mask += padding\n",
    "            segment_ids += padding\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
    "\n",
    "        label = example.label\n",
    "        if example_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(f\"qid: {example.qid}\")\n",
    "            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n",
    "                logger.info(f\"choice: {choice_idx}\")\n",
    "                logger.info(f\"tokens: {' '.join(tokens)}\")\n",
    "                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n",
    "                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n",
    "                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n",
    "            if is_training:\n",
    "                logger.info(f\"label: {label}\")\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_id = example.qid,\n",
    "                choices_features = choices_features,\n",
    "                label = label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "def select_field(features, field):\n",
    "    \"\"\"Yields a list, length equal to the total number of examples,\n",
    "    where each item is a list of arrays,\n",
    "    each array representing the feature array\"\"\"\n",
    "    return [\n",
    "        [\n",
    "            choice[field]   # Grab the feature array of that choice.\n",
    "            for choice in feature.choices_features  # Loop through 5 choices of that example\n",
    "        ]\n",
    "        for feature in features   # loop through each example\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process inputs \n",
    "max_seq_length=50\n",
    "tiny_train = train.iloc[0:10]\n",
    "\n",
    "train_examples= process_examples(tiny_train)\n",
    "train_features = convert_examples_to_features(\n",
    "                    examples=train_examples, \n",
    "                    tokenizer=tokenizer, \n",
    "                    max_seq_length=max_seq_length, \n",
    "                    is_training=True)\n",
    "\n",
    "all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)\n",
    "all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)\n",
    "all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)\n",
    "all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see that there are as many items as number of examples \n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each training example has four things\n",
    "# input id, mask, seg id, and label\n",
    "len(train_data[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input ID has a shape of 5 (num. ans choices) by max length \n",
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model='bert-base-uncased'\n",
    "\n",
    "model = BertForMultipleChoice.from_pretrained(bert_model,\n",
    "        num_choices = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No idea what these do right now\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = 10\n",
    "t_total = num_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-2ab407b9ec44>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-2ab407b9ec44>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break here\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "break here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "train_batch_size =10\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    input_ids, input_mask, segment_ids, label_ids = all_input_ids,  all_input_mask,  all_segment_ids,  all_label\n",
    "    \n",
    "    loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "    \n",
    "    loss.backward()\n",
    "    tr_loss += loss.item()\n",
    "    \n",
    "    nb_tr_examples += input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "        if args.fp16 or args.optimize_on_cpu:\n",
    "            if args.fp16 and args.loss_scale != 1.0:\n",
    "                # scale down gradients for fp16 training\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.data = param.grad.data / args.loss_scale\n",
    "            is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "            if is_nan:\n",
    "                logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                args.loss_scale = args.loss_scale / 2\n",
    "                model.zero_grad()\n",
    "                continue\n",
    "            optimizer.step()\n",
    "            copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        model.zero_grad()\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process dev and training data. We'll just do the first few to make a tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_eg, dev_encoded_eg, dev_labs = process_examples(dev[0:2])\n",
    "train_eg, train_encoded_eg, train_labs = process_examples(train.iloc[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game plan\n",
    "\n",
    "I'll be following the tutorial on: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/, but for specifically commonsenseQA. Here are the steps from the tutorial.\n",
    "\n",
    "1. Embed all sentences. Let's look at the output from the BERT model for our inputs. \n",
    "2. The tutorial says do a train/test split. We don't need this 'cause our data came separate.\n",
    "3. Train the logistic regressio model using the training set. This is training on the output of BERT. I will need to create a FFNN to attach at the end of BERT. See what it comes back with. \n",
    "4. (Optional for now) For each question, pick the answer with the highest score. \n",
    "5. Then, evaluate against true answers. The evaluation metric will be % of questions with the correct answers out of all questions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Embed all sentences. \n",
    "\n",
    "I've already formatted the CommonsenseQA inpts to be fed into the BERT model. Let's look at the output from the BERT model for our inputs. It should have a 768-long vector for each input token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunday morning retry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'oob_bert_classify_{runtype}_{ts}'.format(runtype=runtype, ts=int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='models/logs/{}'.format(NAME))\n",
    "\n",
    "def classification_model(max_len):\n",
    "    \"\"\"Implementation of classification model.\n",
    "    Returns: model\"\"\"\n",
    "    ## BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    ## QA Model\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"InputID\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"AttentionMask\")\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"TokenTypeID\")\n",
    "    embedding = encoder(\n",
    "        [input_ids, attention_mask, token_type_ids]\n",
    "    )[0]\n",
    "    # Feed inputs through the bert model, \n",
    "    # then take just the vector associated with first token [CLS]\n",
    "    bert_cls_output = embedding[:,0]\n",
    "    # These are the layers that come after Bert.\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_cls_output)\n",
    "    # Output layer to predict correct answer. \n",
    "    # For the future, we may modify it to choose the max candidate answer of each question\n",
    "    # for now, just predict from 0 to 1 whether this looks like a correct answer. \n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid', name='correct')(dense)\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, token_type_ids, attention_mask],\n",
    "                                  outputs=pred)\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "On training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_eg, train_encoded_eg, train_labs \n",
    "\n",
    "max_length= len(train_encoded_eg[0])   # num max token \n",
    "BertClassifierModel = classification_model(max_len=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "BertClassifierModel.fit(\n",
    "    [   train_encoded_eg[0],\n",
    "        train_encoded_eg[1],\n",
    "        train_encoded_eg[2]],\n",
    "    train_labs, \n",
    "    epochs=3,\n",
    "    # Insert validation data\n",
    "    validation_data=(\n",
    "        [dev_encoded_eg[0],\n",
    "         dev_encoded_eg[1],\n",
    "         dev_encoded_eg[2]\n",
    "        ], dev_labs\n",
    "    ),\n",
    "    # Log the training info on tensorboard\n",
    "    callbacks=[tensorboard])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution duration in minutes:\", (end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = BertClassifierModel.predict(\n",
    "    [dev_encoded_eg[0],\n",
    "         dev_encoded_eg[1],\n",
    "         dev_encoded_eg[2]\n",
    "        ])\n",
    "\n",
    "location=\"models/predictions/BertClassifierModel_{}_dev_predictions\".format(runtype)\n",
    "pickle_out = open(location, \"wb\")\n",
    "pickle.dump(predictions, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location='models/{NAME}.h5'.format(NAME=NAME)\n",
    "\n",
    "BertClassifierModel.save(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
